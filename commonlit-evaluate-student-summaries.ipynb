{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-13T02:31:38.577602Z","iopub.status.busy":"2023-09-13T02:31:38.577189Z","iopub.status.idle":"2023-09-13T02:31:39.053287Z","shell.execute_reply":"2023-09-13T02:31:39.052035Z","shell.execute_reply.started":"2023-09-13T02:31:38.577569Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-13T02:37:09.809397Z","iopub.status.busy":"2023-09-13T02:37:09.809002Z","iopub.status.idle":"2023-09-13T02:37:09.829209Z","shell.execute_reply":"2023-09-13T02:37:09.828001Z","shell.execute_reply.started":"2023-09-13T02:37:09.809354Z"},"trusted":true},"outputs":[],"source":["# data overview\n","sample_submission = pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-13T02:37:18.865312Z","iopub.status.busy":"2023-09-13T02:37:18.864897Z","iopub.status.idle":"2023-09-13T02:37:18.893692Z","shell.execute_reply":"2023-09-13T02:37:18.892462Z","shell.execute_reply.started":"2023-09-13T02:37:18.86528Z"},"trusted":true},"outputs":[],"source":["sample_submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-13T02:38:22.094175Z","iopub.status.busy":"2023-09-13T02:38:22.093775Z","iopub.status.idle":"2023-09-13T02:38:22.10203Z","shell.execute_reply":"2023-09-13T02:38:22.100818Z","shell.execute_reply.started":"2023-09-13T02:38:22.094137Z"},"trusted":true},"outputs":[],"source":["sample_submission.to_csv(\"/kaggle/working/submission.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Second submission"]},{"cell_type":"markdown","metadata":{},"source":["## Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import zipfile\n","import pandas as pd\n","#import transformers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_in_zip(name):\n","    with zipfile.ZipFile(\"data/commonlit-evaluate-student-summaries.zip\") as z:\n","        with z.open(name) as f:\n","            df = pd.read_csv(f)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","alpaca_model = transformers.AutoModelForCausalLM.from_pretrained(\"/kaggle/input/alpaca/pytorch/7b-wdiff/1\")\n","alpaca_tokenizer = transformers.AutoTokenizer.from_pretrained(\"/kaggle/input/alpaca/pytorch/7b-wdiff/1\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from getpass import getpass\n","\n","HUGGINGFACEHUB_API_TOKEN = getpass()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from langchain.llms import HuggingFaceHub"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["question = summaries_train[\"text\"][3]\n","\n","template = \"\"\"Student Summary: {question}\n","\n","What is the quality in the last student summary? Answer <High>, <Medium> or <Low>\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["repo_id = \"google/flan-t5-base\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llm = HuggingFaceHub(\n","    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",")\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","\n","print(llm_chain.run(question))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["name = \"summaries_test.csv\"\n","summaries_test = read_in_zip(name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summaries_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summaries_test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["name = \"summaries_train.csv\"\n","summaries_train = read_in_zip(name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summaries_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summaries_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summaries_train[\"text\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n","#from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n","#from langchain.llms import OpenAI\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n","llm = HuggingFaceHub(\n","    repo_id=repo_id, model_kwargs={\"temperature\": 0.5}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# How you would like your response structured. This is basically a fancy prompt template\n","response_schemas = [\n","    ResponseSchema(name=\"student_resume\", description=\"This the student resume provided by the user\"),\n","    ResponseSchema(name=\"quality\", description=\"This is your response, the resume quality\")\n","]\n","\n","# How you would like to parse your output\n","output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# See the prompt template you created for formatting\n","format_instructions = output_parser.get_format_instructions()\n","print (format_instructions)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["template = \"\"\"\n","You will be given a student summary from a user.\n","Answer the student resume you were provided.\n","\n","{format_instructions}\n","\n","% USER INPUT:\n","{user_input}\n","\n","YOUR RESPONSE:\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"user_input\"],\n","    partial_variables={\"format_instructions\": format_instructions},\n","    template=template\n",")\n","\n","promptValue = prompt.format(user_input=summaries_train[\"text\"][0])\n","\n","print(promptValue)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llm_output = llm(promptValue)\n","llm_output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","output_parser.parse(llm_output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
